{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bodorcy/hazifeladatok/blob/main/ml_4_feature_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri9YzfKIYAz7"
      },
      "source": [
        "# Szöveges dokumentumok osztályozása\n",
        "Olvasd el a [szövegbányászat](https://inf.u-szeged.hu/~rfarkas/ML20/NLP.html) előadás olvasóleckét."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Xeb0VSGgoH"
      },
      "source": [
        "Építsünk egy osztályozó gépi tanuló algoritmust, amit tanítunk, majd képes lesz korábban nem látott szövegekről döntést hozni, hogy azok pozitív, negatív vagy semleges véleményt fejeznek-e ki!\n",
        "\n",
        "Töltsük le a tanító adatbázist:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXe8loreG2Fq",
        "collapsed": true
      },
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/train.tsv')\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSF13rEOOMkQ"
      },
      "source": [
        "Hibaüzent :( Mi a hiba?\n",
        "\n",
        "Ha a fájlt megnyitjuk akokr látjuk, hogy 3 oszlopot tartalmaz tabbal elválasztva (ezért tsv a kiterjesztés). A pandas.read_csv alapesetben vesszővel elválasztva várja a mezőket.\n",
        "\n",
        "'Expected 2 fields in line 10, saw 4' Az első sor alapján a read_csv úgy gondolta, hogy két oszlop lesz, de a 10. sorban 4 oszlopot talált mert ott vesszők szerepeltek a szövegben...\n",
        "\n",
        "**Érdemes rápillantani egy input filera mielőtt feldolgozni kezdjük!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zefq3cRBG5uP",
        "collapsed": true
      },
      "source": [
        "# ha a read_csv-nek megadjuk, hogy tab a separator akkor minden helyesen működik\n",
        "train_data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/train.tsv', sep='\\t')\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsG8pHNERI-F"
      },
      "source": [
        "Értsük meg mi van az adatbázisban! Például milyen címkék (label) találhatóak benne?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A56Kn0HdICb9"
      },
      "source": [
        "train_data.label.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B6sfsQq2JM6"
      },
      "source": [
        "## Szózsák jellemzőtér\n",
        "\n",
        "A dokumentumok lesznek a gépi tanulásban az egyedek. A legegyszerűbb jellemzőkészket amivel reprezentálni lehet egy dokumentumot az az egyes dokumentumban előforduló szavak gyakorisága. Minden az adatbázisban előforduló szóra felveszünk egy jellemzőt. Tfh az 'alma' szó szerepel a szótárunkban. Ekkor ha a dokumentumban kétszer fordul elő, akkor a jellemző értéke 2 lesz, ha pedig nem fordul elő akkor 0.\n",
        "\n",
        "Ezt **szózsák reprezentációnak**nek (bag-of-words) is hívják, mivel olyan mintha a szavakat beöntenénk egy zsákba, elveszik azok sorrendisége (pl. nem mindegy 'not' és 'good' szavak egymás után fordultak-e elő) és pozíciója. De első gépi tanulási kísérletre pont jó :)\n",
        "\n",
        "Az `sklearn`ben a szózsákot a `CountVectorizer` jelemzőkinyerő implementálja. Ez leszámolja a dokumentumok szavait és azokat `sklearn` jellemzőkké (feature) alakítja."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qFWZYLKGjgg"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK-eL8RMUdhF"
      },
      "source": [
        "# fit: összegyűjti a szókincset, azaz minden tokent ami legalább egyszer előfordul az adatbázisban\n",
        "vectorizer.fit(train_data.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlECX8K2UoZa"
      },
      "source": [
        "len(vectorizer.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaCX_5qoVoWj",
        "collapsed": true
      },
      "source": [
        "vectorizer.vocabulary_ #ua, de itt a \"szókincs\" a jellemzőtér"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T2jaw6eYAH7"
      },
      "source": [
        "# transform: végrehajtja a jellemzőkinyerést, azaz minden dokumentumhoz leszámolja minden szótárbeli szó gyakoriságát\n",
        "features = vectorizer.transform(train_data.text)\n",
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxld_KigYNUC"
      },
      "source": [
        "A tanító adatbázisunk így 9063 egyedet tartalmaz (sorok) és 24285 jellemzőt (oszlopot). Azaz 24285 különböző token alkotja a szótárat. Mivel egy dokumentumban nagyon kevés szó fordul elő a 24285 szóból, ezért a jellemző mátrix túlnyomó része 0, ritka mátrixot használunk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_ib7EiYGuve"
      },
      "source": [
        "# fit_transform: a fit és transform egymás után futtatva\n",
        "features = vectorizer.fit_transform(train_data.text)\n",
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D602ic3y2iMx"
      },
      "source": [
        "## Lineáris gépek\n",
        "\n",
        "**Tanítunk** egy ún. [lineáris gép osztályozó](https://inf.u-szeged.hu/~rfarkas/ML20/linearis_gep.html) modellt, a [stochastic gradient descend (SGD)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html). Itt az elöző órán megismert döntési fát nem szerencsés használni mert nagyon sok jellemzőnk van és azok között megtanulni a kapcsolatot nagyon sok idő. Ilyenkor [lineáris osztályozókat](https://scikit-learn.org/stable/modules/linear_model.html) (SGD mellett például Logisztikus Regresszió) érdemes használni vagy olyan döntési fa variánsokat amik sok jellemzőre lettek kitalálva (pl. [xgboost](https://xgboost.readthedocs.io/))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqjjGTs5G-AF"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "cls = SGDClassifier()\n",
        "model = cls.fit(features, train_data.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp_zjvCKbVVP"
      },
      "source": [
        "model.coef_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU_drPTMeVdJ"
      },
      "source": [
        "Az egyes osztályok legerősebb jellemzői:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXAvVXNqdPZl"
      },
      "source": [
        "sorted(zip(model.coef_[2], vectorizer.get_feature_names_out()),reverse=False)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vizualizáljuk** a legerősebb jellemzőket!"
      ],
      "metadata": {
        "id": "ZjVW9IN3qSuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Gosf6FAKHLTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vectorizer.get_feature_names_out()\n",
        "feature_weights = cls.coef_"
      ],
      "metadata": {
        "id": "HC2XcEhUC-Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = np.concatenate([[feature_names], feature_weights]).T\n",
        "columns = [\"words\"] + cls.classes_.tolist()"
      ],
      "metadata": {
        "id": "SPbTFLSOIfhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = pd.DataFrame(rows, columns=columns)\n",
        "feature_df"
      ],
      "metadata": {
        "id": "4Z2ugkMDGGZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for class_label in cls.classes_:\n",
        "  feature_df.sort_values(class_label, ascending=False)[:10].plot(kind=\"bar\", x=\"words\", y=class_label)"
      ],
      "metadata": {
        "id": "Ik9YZRDtMLtS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxY20CKMZDWl"
      },
      "source": [
        "**Értékeljük ki**, hogy milyen pontos modellt tanítottunk ezen az adatbázison!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp6hlCx3HFM0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyZq5H8dHCG1"
      },
      "source": [
        "prediction = model.predict(features)\n",
        "prediction # a predikció eredménye egy lista az egyedekre predikált címkékkel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7NNvnICZSFy"
      },
      "source": [
        "accuracy_score(y_true=train_data.label, y_pred=prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbIUkSVTaU90"
      },
      "source": [
        "print(classification_report(y_true=train_data.label, y_pred=prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djSJr4W5cbWE"
      },
      "source": [
        "Megjegyzés: a tanító adatbázison való kiértékelésnek is van haszna. Ha nagyon alacsony értékeket kapunk az azt jelenti, hogy vagy jellemzőkészlet amit kinyertünk nem tartalmaz a tanuláshoz elég információt vagy a gépi tanuló modellt rosszul választottuk meg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8A6vxVON1ZI"
      },
      "source": [
        "test_data = pd.read_csv('https://raw.githubusercontent.com/rfarkas/student_data/main/sentiment/test.tsv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irwbCP17d0kO"
      },
      "source": [
        "vectorizer2 = CountVectorizer()\n",
        "test_features = vectorizer2.fit_transform(test_data.text)\n",
        "prediction = model.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdI5kknoeJBo"
      },
      "source": [
        "Hibaüzenet :( Már megint...\n",
        "\n",
        "`'X has 7271 features per sample; expecting 24285'`\n",
        "\n",
        "Emlékezzünk vissza, hogy a `vectorizer.fit_transform` `fit`je először az adatbázisból elkészíti a szótárat (legalább egyszer előforduló szavak az egész adatbázisban). A test adatbázisban 7271 különböző szó fordult elő. Így a test adatbázis jellemzőkészlete (`test_features`) nem kompatibilis a `model` jellemzőkészletével ami 24285 dimenziós. Fontos, hogy a tanító adatbázison kialakított jellemzőkészletet használjuk a test adatbázison is!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = vectorizer.transform(test_data.text)\n",
        "prediction = model.predict(test_features)"
      ],
      "metadata": {
        "id": "hZ_GUl4Oa0gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features"
      ],
      "metadata": {
        "id": "kuALwOhrbGgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_wzgUUWgEs9"
      },
      "source": [
        "accuracy_score(y_true=test_data.label, y_pred=prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkunSTuCgqsi"
      },
      "source": [
        "Jelen esetben a konstans baseline accuracy-ja 0.333 lenne, úgyhogy kijelenthetjük, hogy a modell tanult valamit :)\n",
        "\n",
        "De azért a baseline-t mérjük ki hivatalosan is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9XD4gvIXEJy"
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\") # tanító adatbázis leggyakoribb osztálya lesz mindig a predikció\n",
        "dummy_clf.fit(features, train_data.label) # ugyanazon a tanító adatbázison \"tanítjuk\"\n",
        "baseline_prediction = dummy_clf.predict(test_features) # predikció a kiértékelő adatbázison\n",
        "accuracy_score(baseline_prediction, test_data.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdKx8pjNo20b"
      },
      "source": [
        "## Az SGDClassifier-ben a loss és az alpha beállításával tudjuk elkerülni a túltanulást\n",
        "## Ezt hívjuk regularizációnak lineáris gépeknél\n",
        "cls = SGDClassifier(alpha=0.001)\n",
        "model = cls.fit(features, train_data.label)\n",
        "prediction = model.predict(test_features)\n",
        "accuracy_score(y_true=test_data.label, y_pred=prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#egy másik lineáris gép osztályozó\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "cls = LogisticRegression()"
      ],
      "metadata": {
        "id": "jq8raIg1Jizo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJIvdEjohmqI"
      },
      "source": [
        "## Tévesztési mátrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-DqVQ_ZgVZy"
      },
      "source": [
        "print(classification_report(y_true=test_data.label, y_pred=prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS1MtekQg9WG"
      },
      "source": [
        "Látszik, hogy a negatív osztályt valamiért könnyebben megtanulja, mind a precision, mind a recall jobb ott, mint a másik két osztályon...\n",
        "\n",
        "A tévesztési (confusion) mátrix megmutatja, hogy melyik osztályt melyik másik osztállyal téveszti össze a modell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQFYKbzVYGiY"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_true=test_data.label, y_pred=prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li_n5A7bZFSl"
      },
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_true=test_data.label, y_pred=prediction)\n",
        "ConfusionMatrixDisplay(cm, display_labels=model.classes_).plot(values_format='.3g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXCQMyYHbFUr"
      },
      "source": [
        "A pozitív és semleges osztályokat valamiért sokszor összekeveri... Nézzünk rá példákat a tanító adatbázison!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2uti9Y4hIKb"
      },
      "source": [
        "# Modell javítása új jellemzőkészlettel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUNg3YOW2tJH"
      },
      "source": [
        "Ha a szavak mellett szópárok (bigram) is jellemzők lennének, több információt adunk át a modellnek. Így például a 'not good' megjelenik jellemzőként és esélyt adunk a modellnek, hogy összefüggést tanuljon rá."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVRibkGVHNOz"
      },
      "source": [
        "# CountVectorizernek átparaméterezésével új jellemzőkinyerést valósítunk meg, minden más ugyanaz marad\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=2)\n",
        "# ngram_range mondja meg, hogy szavakat (unigram, 1gram) és egymás utáni szavakból álló párokat (bigram, 2gram) használjunk\n",
        "# min_df=2 eldobja azokat a szavakat amik kevesebb, mint 2 dokumentumban fordult elő. Nagyon sok bigram van, ezzel csökkentjük a jellemzőkészlet dimenziószámát\n",
        "# CountVectorizernek számos paramétere van, lásd: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "features_bigram = bigram_vectorizer.fit_transform(train_data.text)\n",
        "\n",
        "cls = SGDClassifier(alpha=0.001) # új, üres osztályozó\n",
        "model_bigram = cls.fit(features_bigram, train_data.label)\n",
        "features_bigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmWh1vCXhT91"
      },
      "source": [
        "bigram_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeUqykC2HhAP"
      },
      "source": [
        "prediction_bigram = model_bigram.predict(bigram_vectorizer.transform(test_data.text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElFRrdOHHlC3"
      },
      "source": [
        "print(accuracy_score(y_true=test_data.label, y_pred=prediction_bigram))\n",
        "print(classification_report(y_true=test_data.label, y_pred=prediction_bigram))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJf00rXc24QM"
      },
      "source": [
        "Vannak olyan szavak amelyek sokszor fordulnak elő szövegekben, de szétválasztó erejük nincs, azaz ugyanúgy előfordulnak minden osztályban. Ilyenek például, sok más közt, a névelők. Ezek félreviszik az osztályozást (zajra tanulunk rá). A legegyszerűbb technika ennek kiküszöbölésére az ha a sima szógyakoriság (term frequency, TF) helyett normalizáljunk a szavak dokumentumok feletti gyakoriságávl (inverse document frequency, IDF). Lásd:  [TfIdf](http://www.tfidf.com)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAwnUHWGIOXK"
      },
      "source": [
        "# TfidfTransformer is egy új ellemzőkinyersi mód, minden más változatlan\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "vectorizer = CountVectorizer()\n",
        "cv_counts = vectorizer.fit_transform(train_data.text)\n",
        "idf_transformer = TfidfTransformer(use_idf=True).fit(cv_counts)\n",
        "features_idf = idf_transformer.transform(cv_counts)\n",
        "features_idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89WWM8TxMcKE"
      },
      "source": [
        "cls = SGDClassifier()\n",
        "model_idf = cls.fit(features_idf, train_data.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqPGSHXYM05i"
      },
      "source": [
        "prediction_idf = model_idf.predict(idf_transformer.transform(vectorizer.transform(test_data.text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBbSYmDuM7mn"
      },
      "source": [
        "print(accuracy_score(y_true=test_data.label, y_pred=prediction_idf))\n",
        "print(classification_report(y_true=test_data.label, y_pred=prediction_idf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB-vfm69jdHZ"
      },
      "source": [
        "Itt már kézzel fogható javulást tudtunk elérni."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu0uY9rRhoza"
      },
      "source": [
        "# Model javítása előfeldolgozással"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kahmvrchse6"
      },
      "source": [
        "def preprocess(textcol):\n",
        "    return textcol.replace('\\d+', 'NUM',regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcgcnsDNiZHv"
      },
      "source": [
        "train_data.text = preprocess(train_data.text)\n",
        "test_data.text  = preprocess(test_data.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6xfrRkKkdC3"
      },
      "source": [
        "train_data.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDN511cLB_6O"
      },
      "source": [
        "Az [NLTK](https://www.nltk.org/) az egyik leggyakrabban használt python csomag szövegfeldolgozásban. A másik a [spaCy](https://spacy.io/). Kettejük egy összehasonlítása [itt](https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwDwysyN6lie"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wz1PZopH8HO"
      },
      "source": [
        "Miért nem egyértelmű a tokenizálás? Miért léteznek különböző algoritmusok?\n",
        "\n",
        "Azért mert különböző nyelveken másképp lehetnek a szóhatárok (magyar \"-e\"), például rövidítést jelentő pont a token része (\"U.S.A.\" vagy \"kft.\"), míg mondatvégi a írásjel nem. Továbbá a szöveg típusa is megkövetelhet különböző tokenizálókat, pl. szociális médiában az emotikonok és URLeket egyben kell tartni de a camelcase szavakat (pl. JoMunkahozIdoKell) tokenizáljuk, kémiai szövegekben a kötőjel egy molekula nevében nem szóhatár, stb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRiQX0N17QDd"
      },
      "source": [
        "text = \"\"\"What can I say about this place. The staff of the restaurant is nice and the eggplant is not bad. Apart from that, very uninspired food, lack of atmosphere and too expensive. I am a staunch vegetarian and was sorely dissapointed with the veggie options on the menu. Will be the last time I visit, I recommend others to avoid.\"\"\"\n",
        "\n",
        "nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle') # Számos szöveget mondatra bontó algoritmus (splitter) van implementálva az NLTKban. A Punkt az egyik, ezt betöltjük.\n",
        "nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer() # Számos szavakra (tokenekre) bontó algoritmus (tokenizer) is implementálva van.\n",
        "\n",
        "# text mondatai:\n",
        "sentences = nltk_splitter.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1-E967iF56P"
      },
      "source": [
        "Sajnos ez így még nem működik. Az NLTK erőforrásfájlait külön le kell töltenünk a Colab-ba. Ha azok nincsenek ott hibaüzenettel elszállunk futtatás közben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUgfAElXEdKl"
      },
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkRYOBxUHCol"
      },
      "source": [
        "Most már működik! Futassuk újra a nltk_splitter inicializáló kódcellát!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjDK2aQtDjlr"
      },
      "source": [
        "sentences = nltk_splitter.tokenize(text) # a szöveget mondatokra bontjuk\n",
        "sentences # mondatokat tartalmazó lista"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjcEAG1bHUQv"
      },
      "source": [
        "tokenized_sentences = [nltk_tokenizer.tokenize(sent) for sent in sentences] # a mondatokat bejárjuk a for ciklussal és egyessével tokenizáljuk őket\n",
        "tokenized_sentences # listák listája, amiben a mondatok szavai vannak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rnGjGntD14V"
      },
      "source": [
        "Egy másik fontos előfeldolgozási lépés a **szótövesítés (stemming, lemmatizáció)**. A cél, hogy a szavak különböző ragozott alakjait össze tudjuk vonni (pl. általában nem érdemes külön kezelni az 'asztalaitokra' és 'asztal' szóalakokat). A lemmatizáció az igazi nyelvtani értelemben vett szótő meghatározását jelenti. Ez nagyon bonyolult feladat tud lenni bizonyos nyelveken, pl. a magyarban ahol tőhangváltás is van (a 'madarak' szó szótöve a 'madár'). De sokszor elég a szótőnek egy \"közelítése\", azaz egyszerű szabályokkal lecseréljük a szóalak végi karaktereket más karakterre. Ezt a közelítő (butább, de sokkal egyszerűbb és gyorsabb) hívjuk stemmelésnek ([részletesen](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paaVBVZcD5wN"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer # egy angol igazi lemmatizáló\n",
        "nltk.download('wordnet') # erőforrást itt is le kell tölteni hozzá\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLxaNRGlEREe"
      },
      "source": [
        "lemmatizer.lemmatize(\"companies\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHEWwu8mF6aD"
      },
      "source": [
        "from nltk.stem import PorterStemmer # egy angol gyors stemmer (nincs erőforrásfájl, a szabályok a kódban vannk)\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oB5uZ-mGJyU"
      },
      "source": [
        "stemmer.stem(\"companies\") # 'es' végződés levágása sokszor működik többesszámú főneveknél. A 'companies' esetén \"buta\"."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBHAiqDQoYfZ"
      },
      "source": [
        "# Gyakorló feladatok\n",
        "\n",
        "Az órai adatbázison hajts végre egy kísérletet (tanítás, predikció és kiértékelés) ahol\n",
        "\n",
        "*   a szavak szótövét vagy stemjét használjuk a szózsák modellben!\n",
        "*   egy másik lineáris gépet, a Logisztkus Regresszió osztályozó algoritmust használunk (Logistic Regression Classifier).\n",
        "\n",
        "Írd ki, hogy mekkora szótár lesz így illetve mennyi így az accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7frrQF2lqPM"
      },
      "source": [
        "---\n",
        "**Következő két hétben:**\n",
        "Az elmúlt 10+ évben a szózsák modellt felváltották a szóbeágyazás alapú reprezentációk, pl. [word2vec](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial) illetve az ezeket használó deep learning osztályozók, konvolúciós és rekurrens neurális hálózatok. Aztán 2019 óta nagy nyelvi modellek nyertek teret, mint a BERT és a GPT"
      ]
    }
  ]
}